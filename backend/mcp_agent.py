"""
MCPæ™ºèƒ½ä½“å°è£… - ä¸ºWebåç«¯ä½¿ç”¨
åŸºäº test.py ä¸­çš„ SimpleMCPAgentï¼Œä¼˜åŒ–ä¸ºé€‚åˆWebSocketæµå¼æ¨é€çš„ç‰ˆæœ¬
"""

import os
import json
import asyncio
from typing import Dict, List, Any, AsyncGenerator, Optional
from pathlib import Path
from datetime import datetime, timedelta

from dotenv import load_dotenv, find_dotenv
import re
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field
import contextvars

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. MCPé…ç½®ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MCPConfig:
    """MCPé…ç½®ç®¡ç†"""

    def __init__(self, config_file: str = "mcp.json"):
        self.config_file = config_file
        self.default_config = {}

    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if Path(self.config_file).exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        self.save_config(self.default_config)
        return self.default_config

    def save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Webç‰ˆMCPæ™ºèƒ½ä½“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WebMCPAgent:
    """Webç‰ˆMCPæ™ºèƒ½ä½“ - æ”¯æŒæµå¼æ¨é€"""

    def __init__(self):
        # ä¿®å¤ï¼šä½¿ç”¨backendç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent / "mcp.json"
        self.config = MCPConfig(str(config_path))
        self.llm = None
        self.llm_tools = None  # ç»‘å®šå·¥å…·ç”¨äºåˆ¤å®šä¸å·¥å…·é˜¶æ®µ
        self.llm_nontool = None  # æ— å·¥å…·å®ä¾‹ï¼Œä»…ç”¨äºå·¥å…·å†…éƒ¨å¦‚SQLé‡å†™
        self.mcp_client = None
        self.tools = []
        # æ–°å¢ï¼šæŒ‰æœåŠ¡å™¨åˆ†ç»„çš„å·¥å…·å­˜å‚¨
        self.tools_by_server = {}
        self.server_configs = {}
        self._used_tool_names = set()
        # no special exit tool

        # åŠ è½½ .env å¹¶è®¾ç½®APIç¯å¢ƒå˜é‡ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
        try:
            load_dotenv(find_dotenv(), override=True)
        except Exception:
            # å¿½ç•¥ .env åŠ è½½é”™è¯¯ï¼Œç»§ç»­ä»ç³»ç»Ÿç¯å¢ƒè¯»å–
            pass

        # ä»ç¯å¢ƒå˜é‡è¯»å–å•æ¨¡å‹é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        self.api_key = os.getenv("OPENAI_API_KEY", "").strip()
        self.base_url = os.getenv("OPENAI_BASE_URL", "").strip()
        self.model_name = os.getenv("OPENAI_MODEL", os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")).strip()

        # è¯»å–å¤šæ¨¡å‹æ¡£ä½é…ç½®
        self.llm_profiles = self._load_llm_profiles_from_env()
        # é»˜è®¤æ¡£ä½ï¼Œä¼˜å…ˆæ˜¾å¼æŒ‡å®šï¼Œå¦åˆ™ä½¿ç”¨ default
        self.default_profile_id = os.getenv("LLM_DEFAULT", "default").strip() or "default"
        if self.default_profile_id not in self.llm_profiles:
            self.default_profile_id = "default"
        # ç¼“å­˜ä¸åŒæ¡£ä½çš„ LLM å®ä¾‹
        self._llm_cache: Dict[str, Dict[str, Any]] = {}

        # æ•°å€¼é…ç½®ï¼Œå¸¦é»˜è®¤
        try:
            self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
        except Exception:
            self.temperature = 0.2
        try:
            self.timeout = int(os.getenv("OPENAI_TIMEOUT", "60"))
        except Exception:
            self.timeout = 60

        # å°†å…³é”®é…ç½®åŒæ­¥åˆ°ç¯å¢ƒï¼ˆä¾›åº•å±‚SDKä½¿ç”¨ï¼‰ï¼Œä¸è¦†ç›–å¤–éƒ¨å·²è®¾å€¼
        if self.api_key and not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = self.api_key
        if self.base_url and not os.getenv("OPENAI_BASE_URL"):
            os.environ["OPENAI_BASE_URL"] = self.base_url

        # ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆå­˜æ”¾æ¯ä¸ª session çš„åŸºæœ¬ä¿¡æ¯ï¼‰
        self.session_contexts: Dict[str, Dict[str, Any]] = {}

    def _load_llm_profiles_from_env(self) -> Dict[str, Dict[str, Any]]:
        """ä»ç¯å¢ƒå˜é‡è§£æå¤šæ¨¡å‹æ¡£ä½é…ç½®ã€‚
        çº¦å®šï¼š
        - LLM_PROFILES=profile1,profile2
        - æ¯ä¸ªæ¡£ä½å˜é‡ï¼š
          LLM_<ID>_LABELã€LLM_<ID>_API_KEYã€LLM_<ID>_BASE_URLã€LLM_<ID>_MODELã€
          ï¼ˆå¯é€‰ï¼‰LLM_<ID>_TEMPERATUREã€LLM_<ID>_TIMEOUT
        - åŒæ—¶æä¾›ä¸€ä¸ªå‘åå…¼å®¹çš„ default æ¡£ä½ï¼Œæ¥è‡ª OPENAI_* å˜é‡
        """
        profiles: Dict[str, Dict[str, Any]] = {}

        # default æ¡£ä½ï¼ˆå‘åå…¼å®¹ç°æœ‰ OPENAI_*ï¼‰
        profiles["default"] = {
            "id": "default",
            "label": os.getenv("LLM_DEFAULT_LABEL", "Default"),
            "api_key": os.getenv("OPENAI_API_KEY", "").strip(),
            "base_url": os.getenv("OPENAI_BASE_URL", "").strip(),
            "model": os.getenv("OPENAI_MODEL", os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")).strip(),
            "temperature": float(os.getenv("OPENAI_TEMPERATURE", "0.2")),
            "timeout": int(os.getenv("OPENAI_TIMEOUT", "60")),
        }

        ids_raw = os.getenv("LLM_PROFILES", "").strip()
        if ids_raw:
            for pid in [x.strip() for x in ids_raw.split(",") if x.strip()]:
                key = f"LLM_{pid.upper()}_API_KEY"
                model_key = f"LLM_{pid.upper()}_MODEL"
                # æ²¡æœ‰ api_key æˆ– model çš„è·³è¿‡
                api_key = os.getenv(key, "").strip()
                model_name = os.getenv(model_key, "").strip()
                if not api_key or not model_name:
                    continue
                base_url = os.getenv(f"LLM_{pid.upper()}_BASE_URL", "").strip()
                label = os.getenv(f"LLM_{pid.upper()}_LABEL", pid)
                try:
                    temperature = float(os.getenv(f"LLM_{pid.upper()}_TEMPERATURE", os.getenv("OPENAI_TEMPERATURE", "0.2")))
                except Exception:
                    temperature = 0.2
                try:
                    timeout = int(os.getenv(f"LLM_{pid.upper()}_TIMEOUT", os.getenv("OPENAI_TIMEOUT", "60")))
                except Exception:
                    timeout = 60
                profiles[pid] = {
                    "id": pid,
                    "label": label,
                    "api_key": api_key,
                    "base_url": base_url,
                    "model": model_name,
                    "temperature": temperature,
                    "timeout": timeout,
                }

        return profiles

    def get_models_info(self) -> Dict[str, Any]:
        """å¯¹å¤–æš´éœ²çš„æ¨¡å‹æ¡£ä½ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰ã€‚"""
        profiles = self.llm_profiles or {}
        ids = list(profiles.keys())
        non_default_ids = [pid for pid in ids if pid != "default"]

        # è®¡ç®—æœ‰æ•ˆé»˜è®¤æ¡£ä½ï¼šä¼˜å…ˆé‡‡ç”¨ LLM_DEFAULT æŒ‡å®šä¸”å­˜åœ¨çš„IDï¼›
        # å¦åˆ™è‹¥å­˜åœ¨é default æ¡£ä½ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦åˆ™åªèƒ½æ˜¯ defaultï¼ˆå•æ¨¡å‹æ—§å…¼å®¹ï¼‰
        if self.default_profile_id and self.default_profile_id != "default" and self.default_profile_id in profiles:
            effective_default = self.default_profile_id
        elif non_default_ids:
            effective_default = non_default_ids[0]
        else:
            effective_default = "default"

        # å±•ç¤ºç­–ç•¥ï¼š
        # - è‹¥å­˜åœ¨ä»»æ„é default æ¡£ä½ï¼Œåˆ™å®Œå…¨éšè— defaultï¼ˆå®ƒåªä½œä¸ºåˆ«å/å›é€€ï¼Œä¸å•ç‹¬æ˜¾ç¤ºï¼‰ã€‚
        # - è‹¥åªæœ‰ default ä¸€ä¸ªæ¡£ä½ï¼Œåˆ™æ˜¾ç¤ºå®ƒï¼ˆæ—§ç‰ˆå•æ¨¡å‹åœºæ™¯ï¼‰ã€‚
        show_ids = non_default_ids if non_default_ids else (["default"] if "default" in profiles else [])

        # å»é‡æ˜¾ç¤ºï¼šæŒ‰ (base_url, model) ä½œä¸ºç­¾å
        seen_signatures = set()
        models = []
        for pid in show_ids:
            cfg = profiles.get(pid, {})
            signature = (cfg.get("base_url", "").strip(), cfg.get("model", "").strip())
            if signature in seen_signatures:
                continue
            seen_signatures.add(signature)
            models.append({
                "id": pid,
                "label": cfg.get("label", pid),
                "model": cfg.get("model", ""),
                "is_default": pid == effective_default,
            })

        # æç«¯å…œåº•ï¼šå¦‚æœæœ€ç»ˆä¸€ä¸ªéƒ½æ²¡æœ‰ï¼ˆç†è®ºä¸ä¼šå‘ç”Ÿï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ä¸é»˜è®¤ID
        return {"models": models, "default": effective_default}

    def _get_or_create_llm_instances(self, profile_id: str) -> Dict[str, Any]:
        """æ ¹æ®æ¡£ä½è·å–/åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹é›†åˆï¼šllmã€llm_nontoolã€llm_toolsã€‚"""
        pid = profile_id or self.default_profile_id
        if pid not in self.llm_profiles:
            pid = self.default_profile_id

        if pid in self._llm_cache:
            return self._llm_cache[pid]

        cfg = self.llm_profiles[pid]

        # ä¸´æ—¶åˆ‡æ¢ç¯å¢ƒå˜é‡ï¼Œæ„é€ å®ä¾‹
        prev_key = os.getenv("OPENAI_API_KEY")
        prev_base = os.getenv("OPENAI_BASE_URL")
        try:
            if cfg.get("api_key"):
                os.environ["OPENAI_API_KEY"] = cfg["api_key"]
            if cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = cfg["base_url"]

            base_llm = ChatOpenAI(
                model=cfg.get("model", self.model_name),
                temperature=cfg.get("temperature", self.temperature),
                timeout=cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            llm_nontool = ChatOpenAI(
                model=cfg.get("model", self.model_name),
                temperature=cfg.get("temperature", self.temperature),
                timeout=cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            llm_tools = base_llm.bind_tools(self.tools)
        finally:
            # è¿˜åŸç¯å¢ƒï¼Œé¿å…å½±å“å…¶ä»–é€»è¾‘
            if prev_key is not None:
                os.environ["OPENAI_API_KEY"] = prev_key
            if prev_base is not None:
                os.environ["OPENAI_BASE_URL"] = prev_base

        bundle = {"llm": base_llm, "llm_nontool": llm_nontool, "llm_tools": llm_tools}
        self._llm_cache[pid] = bundle
        return bundle

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        try:
            # é€‰æ‹©å¯åŠ¨æ¡£ä½ï¼šä¼˜å…ˆ LLM_DEFAULT æŒ‡å®šçš„æ¡£ä½ï¼›å¦åˆ™é€‰æ‹©ä»»ä¸€å« api_key çš„æ¡£ä½ï¼›
            # è‹¥å‡æ— åˆ™å›é€€åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼›ä»æ— åˆ™æŠ¥é”™
            startup_cfg = None
            # ä¼˜å…ˆé»˜è®¤æ¡£ä½
            if self.default_profile_id in self.llm_profiles:
                cfg = self.llm_profiles[self.default_profile_id]
                if cfg.get("api_key") and cfg.get("model"):
                    startup_cfg = cfg
            # å…¶æ¬¡ä»»æ„æœ‰æ•ˆæ¡£ä½
            if startup_cfg is None:
                for _pid, cfg in self.llm_profiles.items():
                    if _pid == "default":
                        continue
                    if cfg.get("api_key") and cfg.get("model"):
                        startup_cfg = cfg
                        break
            # æœ€åå›é€€åˆ°ç¯å¢ƒå˜é‡
            if startup_cfg is None and os.getenv("OPENAI_API_KEY"):
                startup_cfg = {
                    "api_key": os.getenv("OPENAI_API_KEY").strip(),
                    "base_url": os.getenv("OPENAI_BASE_URL", "").strip(),
                    "model": self.model_name,
                    "temperature": self.temperature,
                    "timeout": self.timeout,
                }
            if startup_cfg is None:
                raise RuntimeError("ç¼ºå°‘å¯ç”¨çš„æ¨¡å‹æ¡£ä½æˆ– OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½® LLM_PROFILES å¯¹åº”çš„ *_API_KEY æˆ–æä¾› OPENAI_API_KEY")

            # ä¸´æ—¶å†™å…¥ç¯å¢ƒä¾›åº•å±‚ SDK ä½¿ç”¨
            if startup_cfg.get("api_key"):
                os.environ["OPENAI_API_KEY"] = startup_cfg["api_key"]
            if startup_cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = startup_cfg["base_url"]

            # ChatOpenAI æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– base_url
            base_llm = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            # ä¸»å¼•ç”¨å‘åå…¼å®¹
            self.llm = base_llm
            # æ— å·¥å…·å®ä¾‹ï¼šå½“å‰ä¸ base_llm ç›¸åŒï¼ˆæ— éœ€ç»‘å®šå·¥å…·ï¼‰ï¼Œä¾›å·¥å…·å†…éƒ¨è°ƒç”¨
            self.llm_nontool = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )

            # åŠ è½½MCPé…ç½®å¹¶è¿æ¥
            mcp_config = self.config.load_config()
            self.server_configs = mcp_config.get("servers", {})

            # å…è®¸æ²¡æœ‰å¤–éƒ¨MCPæœåŠ¡å™¨
            if not self.server_configs:
                print("âš ï¸ æ²¡æœ‰é…ç½®å¤–éƒ¨MCPæœåŠ¡å™¨")
                self.server_configs = {}

            print("ğŸ”— æ­£åœ¨è¿æ¥MCPæœåŠ¡å™¨...")
            
            # å…ˆæµ‹è¯•æœåŠ¡å™¨è¿æ¥
            import aiohttp
            import asyncio
            
            for server_name, server_config in self.server_configs.items():
                try:
                    url = server_config.get('url')
                    if not url:
                        print(f"âš ï¸ æœåŠ¡å™¨ {server_name} ç¼ºå°‘ url é…ç½®ï¼Œè·³è¿‡è¿æ¥æµ‹è¯•")
                        continue
                    print(f"ğŸ§ª æµ‹è¯•è¿æ¥åˆ° {server_name}: {url}")
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            print(f"âœ… {server_name} è¿æ¥æµ‹è¯•æˆåŠŸ (çŠ¶æ€: {response.status})")
                except Exception as test_e:
                    print(f"âš ï¸ {server_name} è¿æ¥æµ‹è¯•å¤±è´¥: {test_e}")
            
            # åˆ›å»ºMCPå®¢æˆ·ç«¯ - å¼ºåˆ¶æ¸…é™¤ç¼“å­˜å¹¶ç¦ç”¨HTTP/2
            import httpx

            def http_client_factory(headers=None, timeout=None, auth=None):
                return httpx.AsyncClient(
                    http2=False,  # ç¦ç”¨HTTP/2
                    headers=headers,
                    timeout=timeout,
                    auth=auth
                )

            # æ›´æ–°æœåŠ¡å™¨é…ç½®ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„httpxå®¢æˆ·ç«¯å·¥å‚
            for server_name in self.server_configs:
                # é¿å…æ±¡æŸ“åŸé…ç½®å¯¹è±¡ï¼Œå¤åˆ¶åæ·»åŠ å·¥å‚
                server_cfg = dict(self.server_configs[server_name])
                server_cfg['httpx_client_factory'] = http_client_factory
                self.server_configs[server_name] = server_cfg

            self.mcp_client = MultiServerMCPClient(self.server_configs)

            # æ”¹ä¸ºä¸²è¡Œè·å–å·¥å…·ï¼Œé¿å…å¹¶å‘é—®é¢˜
            print("ğŸ”§ æ­£åœ¨é€ä¸ªè·å–æœåŠ¡å™¨å·¥å…·...")
            for server_name in self.server_configs.keys():
                try:
                    print(f"â”€â”€â”€ æ­£åœ¨ä»æœåŠ¡å™¨ '{server_name}' è·å–å·¥å…· â”€â”€â”€")
                    server_tools = await self.mcp_client.get_tools(server_name=server_name)
                    # å¯¹å·¥å…·ååšåˆæ³•åŒ–ä¸å»é‡
                    sanitized_tools = []
                    for tool in server_tools:
                        try:
                            original_name = getattr(tool, 'name', '') or ''
                            sanitized = self._sanitize_and_uniq_tool_name(original_name)
                            if sanitized != original_name:
                                print(f"ğŸ§¹ è§„èŒƒåŒ–å·¥å…·å: '{original_name}' -> '{sanitized}'")
                                try:
                                    tool.name = sanitized  # è¦†ç›–åç§°ï¼Œä¾›åç»­ç»‘å®šä¸åŒ¹é…
                                except Exception:
                                    pass
                            sanitized_tools.append(tool)
                        except Exception as _e:
                            print(f"âš ï¸ å·¥å…·åè§„èŒƒåŒ–å¤±è´¥ï¼Œè·³è¿‡: {getattr(tool,'name','<unknown>')} - {_e}")
                            sanitized_tools.append(tool)
                    self.tools.extend(sanitized_tools)
                    self.tools_by_server[server_name] = sanitized_tools
                    print(f"âœ… ä» {server_name} è·å–åˆ° {len(server_tools)} ä¸ªå·¥å…·")
                except Exception as e:
                    print(f"âŒ ä»æœåŠ¡å™¨ '{server_name}' è·å–å·¥å…·å¤±è´¥: {e}")
                    self.tools_by_server[server_name] = []


            # éªŒè¯å·¥å…·æ¥æºï¼Œç¡®ä¿åªæœ‰é…ç½®æ–‡ä»¶ä¸­çš„æœåŠ¡å™¨
            print(f"ğŸ” é…ç½®çš„æœåŠ¡å™¨: {list(self.server_configs.keys())}")
            print(f"ğŸ” å®é™…è·å–åˆ°çš„å·¥å…·æ•°é‡: {len(self.tools)}")
            
            # åˆ†ç»„é€»è¾‘å·²åœ¨ä¸Šé¢çš„å¾ªç¯ä¸­å®Œæˆï¼Œæ— éœ€é¢å¤–è°ƒç”¨

            print(f"âœ… æˆåŠŸè¿æ¥ï¼Œè·å–åˆ° {len(self.tools)} ä¸ªå·¥å…·")
            print(f"ğŸ“Š æœåŠ¡å™¨åˆ†ç»„æƒ…å†µ: {dict((name, len(tools)) for name, tools in self.tools_by_server.items())}")

            # åˆ›å»ºå·¥å…·åˆ¤å®šå®ä¾‹ï¼ˆé»˜è®¤æ¡£ä½ï¼‰ï¼Œå…¶ä½™æ¡£ä½åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶æŒ‰éœ€åˆ›å»º
            self.llm_tools = base_llm.bind_tools(self.tools)

            print("ğŸ¤– Web MCPæ™ºèƒ½åŠ©æ‰‹å·²å¯åŠ¨ï¼")
            return True

        except Exception as e:
            import traceback
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # å°è¯•æ¸…ç†å¯èƒ½çš„è¿æ¥
            if hasattr(self, 'mcp_client') and self.mcp_client:
                try:
                    await self.mcp_client.close()
                except:
                    pass
            return False

    def _get_tools_system_prompt(self) -> str:
        """ç”¨äºå·¥å…·åˆ¤å®š/æ‰§è¡Œé˜¶æ®µçš„ç³»ç»Ÿæç¤ºè¯ï¼šä¸“æ³¨äºæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ä¸å‚æ•°ç”Ÿæˆï¼Œä¸åšæ­£æ–‡åˆ†æè¾“å‡ºã€‚"""
        now = datetime.now()
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
        return (
            f"ä»Šå¤©æ˜¯ {current_date}ï¼ˆ{current_weekday}ï¼‰ã€‚ä½ æ˜¯ä¸€ä¸ªå·¥å…·è°ƒåº¦å™¨ã€‚" + "\n" +
            "- é»˜è®¤ä¸è°ƒç”¨å·¥å…·ã€‚åªæœ‰åœ¨ç¡®å®éœ€è¦ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯æ—¶æ‰è°ƒç”¨ã€‚" + "\n" +
            "- ä¼˜å…ˆç›´æ¥å›ç­”ï¼šå¯¹çº¯æ¨ç†/å¸¸è¯†/æ€»ç»“ç±»è¯·æ±‚ä¸è¦è°ƒç”¨å·¥å…·ã€‚" + "\n" +
            "- ä¸è¦æ— èŠ‚åˆ¶çš„è°ƒç”¨å·¥å…·ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ã€‚" + "\n" +
            "- æ ¹æ®å¯ç”¨å·¥å…·é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚" + "\n" +
            "- ä¸è¦ä¸º'å°è¯•/éªŒè¯'è€Œéšæ„è°ƒç”¨å·¥å…·ï¼›è‹¥ä¿¡æ¯ä¸è¶³ï¼Œè¿”å›ä¸è°ƒç”¨å·¥å…·ã€‚" + "\n" +
            "- ä»…åœ¨ç¡®æœ‰å¿…è¦æ—¶ï¼Œé€šè¿‡ tool_calls ç»™å‡ºå‡½æ•°åä¸'åˆæ³• JSON'å‚æ•°ï¼›ä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚" + "\n"
        )

    def _get_stream_system_prompt(self) -> str:
        """ä¿æŒæ¥å£ä»¥å…¼å®¹æ—§è°ƒç”¨ï¼Œä½†å½“å‰ä¸å†ä½¿ç”¨æµå¼å›ç­”æç¤ºè¯ã€‚"""
        return ""

    def _sanitize_and_uniq_tool_name(self, name: str) -> str:
        """å°†å·¥å…·åè§„èŒƒä¸º ^[a-zA-Z0-9_-]+$ï¼Œå¹¶é¿å…é‡åå†²çªã€‚"""
        if not isinstance(name, str):
            name = str(name or "")
        # ä»…ä¿ç•™å­—æ¯æ•°å­—ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦ï¼Œå…¶ä½™æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", name)
        if not sanitized:
            sanitized = "tool"
        base = sanitized
        # ç¡®ä¿å”¯ä¸€
        index = 1
        while sanitized in self._used_tool_names:
            index += 1
            sanitized = f"{base}_{index}"
        self._used_tool_names.add(sanitized)
        return sanitized

    async def chat_stream(self, user_input: str, history: List[Dict[str, Any]] = None, session_id: Optional[str] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼æ¢æµ‹ + ç«‹å³ä¸­æ–­ï¼š
        - å…ˆç›´æ¥ astream å¼€æµï¼ŒçŸ­æš‚ç¼“å†²å¹¶æ£€æµ‹ function_call/tool_callï¼›
        - è‹¥æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼šç«‹å³ä¸­æ–­æœ¬æ¬¡æµå¼ï¼ˆä¸ä¸‹å‘ç¼“å†²ï¼‰ï¼Œæ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰ï¼Œå†™å› messages åè¿›å…¥ä¸‹ä¸€è½®ï¼›
        - è‹¥æœªæ£€æµ‹åˆ°å·¥å…·ï¼šå°†æœ¬æ¬¡æµä½œä¸ºæœ€ç»ˆå›ç­”ï¼Œå¼€å§‹æµå¼æ¨é€åˆ°ç»“æŸã€‚
        """
        try:
            if session_id:
                try:
                    self._current_session_id_ctx.set(session_id)
                except Exception:
                    pass
            print(f"ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
            yield {"type": "status", "content": "å¼€å§‹ç”Ÿæˆ..."}

            # ä¾æ®ä¼šè¯ä¸Šä¸‹æ–‡é€‰æ‹©æ¨¡å‹æ¡£ä½
            profile_id = None
            try:
                if session_id and self.session_contexts.get(session_id):
                    profile_id = self.session_contexts[session_id].get("model") or self.session_contexts[session_id].get("llm_profile")
            except Exception:
                profile_id = None

            llm_bundle = self._get_or_create_llm_instances(profile_id)
            current_llm_tools = llm_bundle.get("llm_tools", self.llm_tools)

            # 1) æ„å»ºå…±äº«æ¶ˆæ¯å†å²ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºï¼Œä¾¿äºä¸¤å¥—ç³»ç»Ÿæç¤ºåˆ†åˆ«æ³¨å…¥ï¼‰
            shared_history: List[Dict[str, Any]] = []
            if history:
                for record in history:
                    shared_history.append({"role": "user", "content": record['user_input']})
                    if record.get('ai_response'):
                        shared_history.append({"role": "assistant", "content": record['ai_response']})
            shared_history.append({"role": "user", "content": user_input})

            max_rounds = 25
            round_index = 0
            # åˆå¹¶ä¸¤é˜¶æ®µè¾“å‡ºä¸ºåŒä¸€æ¡æ¶ˆæ¯ï¼šåœ¨æ•´ä¸ªä¼šè¯å›ç­”æœŸé—´ä»…å‘é€ä¸€æ¬¡ startï¼Œæœ€åä¸€æ¬¡æ€§ end
            combined_response_started = False
            while round_index < max_rounds:
                round_index += 1
                print(f"ğŸ§  ç¬¬ {round_index} è½®æ¨ç† (åŒå®ä¾‹ï¼šåˆ¤å®šå·¥å…· + çº¯æµå¼å›ç­”)...")

                # 2) ä½¿ç”¨å¸¦å·¥å…·å®ä¾‹åš"æµå¼åˆ¤å®š"ï¼š
                tools_messages = [{"role": "system", "content": self._get_tools_system_prompt()}] + shared_history
                tool_calls_check = None
                buffered_chunks: List[str] = []
                content_preview = ""
                response_started = False
                try:
                    async for event in current_llm_tools.astream_events(tools_messages, version="v1"):
                        ev = event.get("event")
                        if ev == "on_chat_model_stream":
                            data = event.get("data", {})
                            chunk = data.get("chunk")
                            if chunk is None:
                                continue
                            try:
                                content_piece = getattr(chunk, 'content', None)
                            except Exception:
                                content_piece = None
                            if content_piece:
                                # ç«‹å³å‘å‰ç«¯æµå¼ä¸‹å‘ä½œä¸ºæœ€ç»ˆå›å¤ï¼ˆåˆå¹¶æ¨¡å¼ï¼šä»…é¦–æ¬¡å‘é€ startï¼‰
                                if not combined_response_started:
                                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                    combined_response_started = True
                                response_started = True
                                buffered_chunks.append(content_piece)
                                try:
                                    print(f"ğŸ“¤ [åˆ¤å®šLLMæµ] {content_piece}")
                                except Exception:
                                    pass
                                yield {"type": "ai_response_chunk", "content": content_piece}
                        elif ev == "on_chat_model_end":
                            data = event.get("data", {})
                            output = data.get("output")
                            try:
                                tool_calls_check = getattr(output, 'tool_calls', None)
                            except Exception:
                                tool_calls_check = None
                            try:
                                content_preview = getattr(output, 'content', None) or ""
                            except Exception:
                                content_preview = ""
                except Exception as e:
                    print(f"âš ï¸ å·¥å…·åˆ¤å®š(æµå¼)å¤±è´¥ï¼š{e}")
                    tool_calls_check = None
                    content_preview = ""

                if tool_calls_check:
                    # åˆå¹¶æ¨¡å¼ï¼šä¸ç»“æŸæ¶ˆæ¯ï¼Œæ’å…¥åˆ†éš”åç»§ç»­æ‰§è¡Œå·¥å…·ï¼Œæœ€ç»ˆä¸€å¹¶ç»“æŸ
                    if response_started and buffered_chunks:
                        yield {"type": "ai_response_chunk", "content": "\n\n"}
                        buffered_chunks = []

                    tool_calls_to_run = tool_calls_check
                    yield {"type": "tool_plan", "content": f"AIå†³å®šè°ƒç”¨ {len(tool_calls_to_run)} ä¸ªå·¥å…·", "tool_count": len(tool_calls_to_run)}
                    # å†™å›assistantå¸¦tool_calls
                    try:
                        shared_history.append({
                            "role": "assistant",
                            "content": "",
                            "tool_calls": tool_calls_to_run
                        })
                    except Exception:
                        shared_history.append({"role": "assistant", "content": ""})

                    # æ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰
                    exit_to_stream = False
                    for i, tool_call in enumerate(tool_calls_to_run, 1):
                        if isinstance(tool_call, dict):
                            tool_id = tool_call.get('id') or f"call_{i}"
                            fn = tool_call.get('function') or {}
                            tool_name = fn.get('name') or tool_call.get('name') or ''
                            tool_args_raw = fn.get('arguments') or tool_call.get('args') or {}
                        else:
                            tool_id = getattr(tool_call, 'id', None) or f"call_{i}"
                            tool_name = getattr(tool_call, 'name', '') or ''
                            tool_args_raw = getattr(tool_call, 'args', {}) or {}

                        # è§£æå‚æ•°
                        if isinstance(tool_args_raw, str):
                            try:
                                parsed_args = json.loads(tool_args_raw) if tool_args_raw else {}
                            except Exception:
                                parsed_args = {"$raw": tool_args_raw}
                        elif isinstance(tool_args_raw, dict):
                            parsed_args = tool_args_raw
                        else:
                            parsed_args = {"$raw": str(tool_args_raw)}

                        yield {"type": "tool_start", "tool_id": tool_id, "tool_name": tool_name, "tool_args": parsed_args, "progress": f"{i}/{len(tool_calls_to_run)}"}

                        try:
                            target_tool = None
                            for tool in self.tools:
                                if tool.name == tool_name:
                                    target_tool = tool
                                    break
                            if target_tool is None:
                                error_msg = f"å·¥å…· '{tool_name}' æœªæ‰¾åˆ°"
                                print(f"âŒ {error_msg}")
                                yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                                tool_result = f"é”™è¯¯: {error_msg}"
                            else:
                                tool_result = await target_tool.ainvoke(parsed_args)
                                yield {"type": "tool_end", "tool_id": tool_id, "tool_name": tool_name, "result": str(tool_result)}
                                # ä¸å†æ”¯æŒé€€å‡ºå·¥å…·æ¨¡å¼
                        except Exception as e:
                            error_msg = f"å·¥å…·æ‰§è¡Œå‡ºé”™: {e}"
                            print(f"âŒ {error_msg}")
                            yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                            tool_result = f"é”™è¯¯: {error_msg}"

                        # å§‹ç»ˆè¿½åŠ  tool æ¶ˆæ¯ï¼Œæ»¡è¶³ OpenAI å‡½æ•°è°ƒç”¨åè®®è¦æ±‚
                        # å¯¹äºé€€å‡ºå·¥å…·æ¨¡å¼ï¼Œå†…å®¹ä¸ºç®€å•çŠ¶æ€ï¼Œä¸å½±å“åç»­å›ç­”è´¨é‡
                        shared_history.append({
                            "role": "tool",
                            "tool_call_id": tool_id,
                            "name": tool_name,
                            "content": str(tool_result)
                        })

                        if exit_to_stream:
                            break

                    if exit_to_stream:
                        # ä¸å†æ”¯æŒæå‰å¼ºåˆ¶åˆ‡æµå¼ï¼ŒæŒ‰åŸé€»è¾‘ç»§ç»­ä¸‹ä¸€è½®
                        pass
                    else:
                        # å·¥å…·åç»§ç»­ä¸‹ä¸€è½®
                        continue

                # 3) æ— å·¥å…·ï¼šåˆå¹¶æ¨¡å¼
                # è‹¥å…ˆå‰å·²ç»æµå¼è¾“å‡ºè¿‡ç‰‡æ®µï¼Œåˆ™æ­¤å¤„ä¸å†æŠŠæ‰€æœ‰ç‰‡æ®µå†å‘ä¸€æ¬¡ï¼Œåªå‘é€ç»“æŸæ ‡è®°ï¼›
                # è‹¥æ­¤å‰å°šæœªå¼€å§‹ï¼ˆæ— æµå¼ç‰‡æ®µï¼‰ï¼Œåˆ™ä¸€æ¬¡æ€§å‘é€æœ€ç»ˆæ–‡æœ¬å†ç»“æŸã€‚
                final_text = "".join(buffered_chunks) if buffered_chunks else (content_preview or "")
                if combined_response_started:
                    # å·²ç»å¼€å§‹è¿‡ï¼Œé¿å…é‡å¤å†…å®¹
                    yield {"type": "ai_response_end", "content": ""}
                else:
                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                    combined_response_started = True
                    if final_text:
                        try:
                            print(f"ğŸ“¤ [æœ€ç»ˆå›å¤æµ] {final_text}")
                        except Exception:
                            pass
                        yield {"type": "ai_response_chunk", "content": final_text}
                    yield {"type": "ai_response_end", "content": ""}
                return

            # è½®æ¬¡è€—å°½ï¼šç›´æ¥è¿”å›æç¤ºä¿¡æ¯
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°({max_rounds})ï¼Œç›´æ¥è¿”å›æç¤ºä¿¡æ¯")
            final_text = "å·²è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°ï¼Œè¯·ç¼©å°é—®é¢˜èŒƒå›´æˆ–ç¨åé‡è¯•ã€‚"
            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
            yield {"type": "ai_response_chunk", "content": final_text}
            yield {"type": "ai_response_end", "content": final_text}
            return
        except Exception as e:
            import traceback
            print(f"âŒ chat_stream å¼‚å¸¸: {e}")
            print("ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            yield {"type": "error", "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"}

    def get_tools_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰MCPæœåŠ¡å™¨åˆ†ç»„"""
        if not self.tools_by_server:
            return {"servers": {}, "total_tools": 0, "server_count": 0}
        
        servers_info = {}
        total_tools = 0
        
        # æŒ‰æœåŠ¡å™¨åˆ†ç»„æ„å»ºå·¥å…·ä¿¡æ¯
        for server_name, server_tools in self.tools_by_server.items():
            tools_info = []
            
            for tool in server_tools:
                tool_info = {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": {},
                    "required": []
                }
                
                # è·å–å‚æ•°ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬
                try:
                    schema = None
                    
                    # æ–¹æ³•1: å°è¯•ä½¿ç”¨args_schema (LangChainå·¥å…·å¸¸ç”¨)
                    if hasattr(tool, 'args_schema') and tool.args_schema:
                        if isinstance(tool.args_schema, dict):
                            schema = tool.args_schema
                        elif hasattr(tool.args_schema, 'model_json_schema'):
                            schema = tool.args_schema.model_json_schema()
                    
                    # æ–¹æ³•2: å¦‚æœæ²¡æœ‰args_schemaï¼Œå°è¯•tool_call_schema
                    if not schema and hasattr(tool, 'tool_call_schema') and tool.tool_call_schema:
                        schema = tool.tool_call_schema
                    
                    # æ–¹æ³•3: æœ€åå°è¯•input_schema
                    if not schema and hasattr(tool, 'input_schema') and tool.input_schema:
                        if isinstance(tool.input_schema, dict):
                            schema = tool.input_schema
                        elif hasattr(tool.input_schema, 'model_json_schema'):
                            try:
                                schema = tool.input_schema.model_json_schema()
                            except:
                                pass
                    
                    # è§£æschema
                    if schema and isinstance(schema, dict):
                        if 'properties' in schema:
                            tool_info["parameters"] = schema['properties']
                            tool_info["required"] = schema.get('required', [])
                        elif 'type' in schema and schema.get('type') == 'object' and 'properties' in schema:
                            tool_info["parameters"] = schema['properties']
                            tool_info["required"] = schema.get('required', [])
                
                except Exception as e:
                    # å¦‚æœå‡ºé”™ï¼Œè‡³å°‘ä¿ç•™å·¥å…·çš„åŸºæœ¬ä¿¡æ¯
                    print(f"âš ï¸ è·å–å·¥å…· '{tool.name}' å‚æ•°ä¿¡æ¯å¤±è´¥: {e}")
                
                tools_info.append(tool_info)
            
            # æ·»åŠ æœåŠ¡å™¨ä¿¡æ¯
            servers_info[server_name] = {
                "name": server_name,
                "tools": tools_info,
                "tool_count": len(tools_info)
            }
            
            total_tools += len(tools_info)
        
        return {
            "servers": servers_info,
            "total_tools": total_tools,
            "server_count": len(servers_info)
        }

    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self.mcp_client and hasattr(self.mcp_client, 'close'):
                await self.mcp_client.close()
        except:
            pass
