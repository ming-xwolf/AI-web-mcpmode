"""
MCPæ™ºèƒ½ä½“å°è£… - ä¸ºWebåç«¯ä½¿ç”¨
åŸºäº test.py ä¸­çš„ SimpleMCPAgentï¼Œä¼˜åŒ–ä¸ºé€‚åˆWebSocketæµå¼æ¨é€çš„ç‰ˆæœ¬
"""

import os
import json
import asyncio
from typing import Dict, List, Any, AsyncGenerator, Optional
from pathlib import Path
from datetime import datetime, timedelta

from dotenv import load_dotenv, find_dotenv
import re
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. MCPé…ç½®ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MCPConfig:
    """MCPé…ç½®ç®¡ç†"""

    def __init__(self, config_file: str = "mcp.json"):
        self.config_file = config_file
        self.default_config = {}

    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if Path(self.config_file).exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        self.save_config(self.default_config)
        return self.default_config

    def save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Webç‰ˆMCPæ™ºèƒ½ä½“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WebMCPAgent:
    """Webç‰ˆMCPæ™ºèƒ½ä½“ - æ”¯æŒæµå¼æ¨é€"""

    def __init__(self):
        # ä¿®å¤ï¼šä½¿ç”¨backendç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent / "mcp.json"
        self.config = MCPConfig(str(config_path))
        self.llm = None
        self.llm_tools = None  # ç»‘å®šå·¥å…·ç”¨äºåˆ¤å®šä¸å·¥å…·é˜¶æ®µ
        self.llm_stream = None # ä¸ç»‘å®šå·¥å…·ï¼Œä»…ç”¨äºæœ€ç»ˆå›ç­”çš„çœŸæµå¼
        self.mcp_client = None
        self.tools = []
        # æ–°å¢ï¼šæŒ‰æœåŠ¡å™¨åˆ†ç»„çš„å·¥å…·å­˜å‚¨
        self.tools_by_server = {}
        self.server_configs = {}
        self._used_tool_names = set()
        self._exit_tool_name = "exit_tool_mode"

        # åŠ è½½ .env å¹¶è®¾ç½®APIç¯å¢ƒå˜é‡ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
        try:
            load_dotenv(find_dotenv(), override=True)
        except Exception:
            # å¿½ç•¥ .env åŠ è½½é”™è¯¯ï¼Œç»§ç»­ä»ç³»ç»Ÿç¯å¢ƒè¯»å–
            pass

        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY", "").strip()
        self.base_url = os.getenv("OPENAI_BASE_URL", "").strip()
        self.model_name = os.getenv("OPENAI_MODEL", os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")).strip()

        # æ•°å€¼é…ç½®ï¼Œå¸¦é»˜è®¤
        try:
            self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
        except Exception:
            self.temperature = 0.2
        try:
            self.timeout = int(os.getenv("OPENAI_TIMEOUT", "60"))
        except Exception:
            self.timeout = 60

        # å°†å…³é”®é…ç½®åŒæ­¥åˆ°ç¯å¢ƒï¼ˆä¾›åº•å±‚SDKä½¿ç”¨ï¼‰ï¼Œä¸è¦†ç›–å¤–éƒ¨å·²è®¾å€¼
        if self.api_key and not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = self.api_key
        if self.base_url and not os.getenv("OPENAI_BASE_URL"):
            os.environ["OPENAI_BASE_URL"] = self.base_url

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        try:
            # åˆå§‹åŒ–å¤§æ¨¡å‹
            if not os.getenv("OPENAI_API_KEY"):
                raise RuntimeError("ç¼ºå°‘ OPENAI_API_KEYï¼Œè¯·åœ¨ .env æˆ–ç³»ç»Ÿç¯å¢ƒä¸­é…ç½®")

            # ChatOpenAI æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– base_url
            base_llm = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                timeout=self.timeout,
                max_retries=3,
            )
            # ä¸»å¼•ç”¨å‘åå…¼å®¹
            self.llm = base_llm

            # åŠ è½½MCPé…ç½®å¹¶è¿æ¥
            mcp_config = self.config.load_config()
            self.server_configs = mcp_config.get("servers", {})

            if not self.server_configs:
                print("âŒ æ²¡æœ‰é…ç½®MCPæœåŠ¡å™¨")
                return False

            print("ğŸ”— æ­£åœ¨è¿æ¥MCPæœåŠ¡å™¨...")
            
            # å…ˆæµ‹è¯•æœåŠ¡å™¨è¿æ¥
            import aiohttp
            import asyncio
            
            for server_name, server_config in self.server_configs.items():
                try:
                    url = server_config.get('url')
                    if not url:
                        print(f"âš ï¸ æœåŠ¡å™¨ {server_name} ç¼ºå°‘ url é…ç½®ï¼Œè·³è¿‡è¿æ¥æµ‹è¯•")
                        continue
                    print(f"ğŸ§ª æµ‹è¯•è¿æ¥åˆ° {server_name}: {url}")
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            print(f"âœ… {server_name} è¿æ¥æµ‹è¯•æˆåŠŸ (çŠ¶æ€: {response.status})")
                except Exception as test_e:
                    print(f"âš ï¸ {server_name} è¿æ¥æµ‹è¯•å¤±è´¥: {test_e}")
            
            # åˆ›å»ºMCPå®¢æˆ·ç«¯ - å¼ºåˆ¶æ¸…é™¤ç¼“å­˜å¹¶ç¦ç”¨HTTP/2
            import httpx

            def http_client_factory(headers=None, timeout=None, auth=None):
                return httpx.AsyncClient(
                    http2=False,  # ç¦ç”¨HTTP/2
                    headers=headers,
                    timeout=timeout,
                    auth=auth
                )

            # æ›´æ–°æœåŠ¡å™¨é…ç½®ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„httpxå®¢æˆ·ç«¯å·¥å‚
            for server_name in self.server_configs:
                # é¿å…æ±¡æŸ“åŸé…ç½®å¯¹è±¡ï¼Œå¤åˆ¶åæ·»åŠ å·¥å‚
                server_cfg = dict(self.server_configs[server_name])
                server_cfg['httpx_client_factory'] = http_client_factory
                self.server_configs[server_name] = server_cfg

            self.mcp_client = MultiServerMCPClient(self.server_configs)

            # æ”¹ä¸ºä¸²è¡Œè·å–å·¥å…·ï¼Œé¿å…å¹¶å‘é—®é¢˜
            print("ğŸ”§ æ­£åœ¨é€ä¸ªè·å–æœåŠ¡å™¨å·¥å…·...")
            for server_name in self.server_configs.keys():
                try:
                    print(f"â”€â”€â”€ æ­£åœ¨ä»æœåŠ¡å™¨ '{server_name}' è·å–å·¥å…· â”€â”€â”€")
                    server_tools = await self.mcp_client.get_tools(server_name=server_name)
                    # å¯¹å·¥å…·ååšåˆæ³•åŒ–ä¸å»é‡
                    sanitized_tools = []
                    for tool in server_tools:
                        try:
                            original_name = getattr(tool, 'name', '') or ''
                            sanitized = self._sanitize_and_uniq_tool_name(original_name)
                            if sanitized != original_name:
                                print(f"ğŸ§¹ è§„èŒƒåŒ–å·¥å…·å: '{original_name}' -> '{sanitized}'")
                                try:
                                    tool.name = sanitized  # è¦†ç›–åç§°ï¼Œä¾›åç»­ç»‘å®šä¸åŒ¹é…
                                except Exception:
                                    pass
                            sanitized_tools.append(tool)
                        except Exception as _e:
                            print(f"âš ï¸ å·¥å…·åè§„èŒƒåŒ–å¤±è´¥ï¼Œè·³è¿‡: {getattr(tool,'name','<unknown>')} - {_e}")
                            sanitized_tools.append(tool)
                    self.tools.extend(sanitized_tools)
                    self.tools_by_server[server_name] = sanitized_tools
                    print(f"âœ… ä» {server_name} è·å–åˆ° {len(server_tools)} ä¸ªå·¥å…·")
                except Exception as e:
                    print(f"âŒ ä»æœåŠ¡å™¨ '{server_name}' è·å–å·¥å…·å¤±è´¥: {e}")
                    self.tools_by_server[server_name] = []
            
            # æ³¨å…¥æœ¬åœ°â€œé€€å‡ºå·¥å…·æ¨¡å¼â€å·¥å…·ï¼Œä¾›åˆ¤å®šé˜¶æ®µæ˜¾å¼é€€å‡º
            try:
                class ExitToolArgs(BaseModel):
                    reason: Optional[str] = Field(default=None, description="ç®€çŸ­è¯´æ˜ä¸ºä½•é€€å‡ºå·¥å…·æ¨¡å¼")

                def exit_tool_impl(reason: Optional[str] = None) -> Dict[str, Any]:
                    return {"status": "exit", "reason": reason or ""}

                exit_tool = StructuredTool.from_function(
                    func=exit_tool_impl,
                    name=self._exit_tool_name,
                    description="å½“ä½ å†³å®šä¸å†è°ƒç”¨ä»»ä½•å¤–éƒ¨å·¥å…·ã€åº”ç›´æ¥è¿›å…¥å›ç­”é˜¶æ®µæ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·é€šçŸ¥ç³»ç»Ÿé€€å‡ºå·¥å…·æ¨¡å¼ã€‚",
                    args_schema=ExitToolArgs,
                )
                self.tools.append(exit_tool)
                # åˆ†ç»„åˆ°æœ¬åœ°åˆ†ç»„ï¼Œä¾¿äºå‰ç«¯å±•ç¤º
                self.tools_by_server.setdefault("__local__", []).append(exit_tool)
                print(f"ğŸ§° å·²æ³¨å…¥æœ¬åœ°å·¥å…·: {self._exit_tool_name}")
            except Exception as e:
                print(f"âš ï¸ æ³¨å…¥æœ¬åœ°é€€å‡ºå·¥å…·å¤±è´¥: {e}")

            # éªŒè¯å·¥å…·æ¥æºï¼Œç¡®ä¿åªæœ‰é…ç½®æ–‡ä»¶ä¸­çš„æœåŠ¡å™¨
            print(f"ğŸ” é…ç½®çš„æœåŠ¡å™¨: {list(self.server_configs.keys())}")
            print(f"ğŸ” å®é™…è·å–åˆ°çš„å·¥å…·æ•°é‡: {len(self.tools)}")
            
            # åˆ†ç»„é€»è¾‘å·²åœ¨ä¸Šé¢çš„å¾ªç¯ä¸­å®Œæˆï¼Œæ— éœ€é¢å¤–è°ƒç”¨

            print(f"âœ… æˆåŠŸè¿æ¥ï¼Œè·å–åˆ° {len(self.tools)} ä¸ªå·¥å…·")
            print(f"ğŸ“Š æœåŠ¡å™¨åˆ†ç»„æƒ…å†µ: {dict((name, len(tools)) for name, tools in self.tools_by_server.items())}")

            # åˆ›å»ºåŒå®ä¾‹ï¼š
            # 1) å¸¦å·¥å…·å®ä¾‹ï¼šç”¨äºåˆ¤å®šä¸å·¥å…·è°ƒç”¨ï¼ˆéæµå¼é˜¶æ®µï¼‰
            self.llm_tools = base_llm.bind_tools(self.tools)

            # 2) æ— å·¥å…·å®ä¾‹ï¼šç”¨äºæœ€ç»ˆå›ç­”çœŸæµå¼ï¼ˆé¿å…äº§ç”Ÿ tool_calls å¢é‡ï¼‰
            self.llm_stream = ChatOpenAI(
                model=self.model_name,
                temperature=self.temperature,
                timeout=self.timeout,
                max_retries=3,
            )

            print("ğŸ¤– Web MCPæ™ºèƒ½åŠ©æ‰‹å·²å¯åŠ¨ï¼")
            return True

        except Exception as e:
            import traceback
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # å°è¯•æ¸…ç†å¯èƒ½çš„è¿æ¥
            if hasattr(self, 'mcp_client') and self.mcp_client:
                try:
                    await self.mcp_client.close()
                except:
                    pass
            return False

    def _get_tools_system_prompt(self) -> str:
        """ç”¨äºå·¥å…·åˆ¤å®š/æ‰§è¡Œé˜¶æ®µçš„ç³»ç»Ÿæç¤ºè¯ï¼šä¸“æ³¨äºæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ä¸å‚æ•°ç”Ÿæˆï¼Œä¸åšæ­£æ–‡åˆ†æè¾“å‡ºã€‚"""
        now = datetime.now()
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
        return (
            f"ä»Šå¤©æ˜¯ {current_date}ï¼ˆ{current_weekday}ï¼‰ã€‚ä½ æ˜¯ä¸€ä¸ªâ€˜å·¥å…·è°ƒåº¦å™¨â€™ã€‚\n"
            "- ä½ çš„ç›®æ ‡æ˜¯åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Œå¹¶ç»™å‡ºå‡†ç¡®çš„å·¥å…·åç§°å’Œå‚æ•°ï¼ˆJSONï¼‰ã€‚\n"
            "- å¦‚æœéœ€è¦ï¼Œè¯·é€šè¿‡ tool_calls ç»“æ„ä½“ç»™å‡ºå‡½æ•°åä¸æœ‰æ•ˆçš„ JSON å‚æ•°ã€‚\n"
            "- å¦‚æœä¸éœ€è¦å·¥å…·ï¼Œä¸è¦è¾“å‡ºæ­£æ–‡åˆ†æå†…å®¹ã€‚\n"
            "- å‚æ•°å¿…é¡»æ˜¯åˆæ³• JSON å­—å…¸ï¼ˆobjectï¼‰ï¼Œä¸è¦è¾“å‡ºä¸å®Œæ•´çš„ç‰‡æ®µã€‚\n"
            "- ä¸è¦è¾“å‡ºé¢å‘ç”¨æˆ·çš„è§£é‡Šæˆ–åˆ†æï¼Œè¿™ä¸ªç•™ç»™åç»­å›ç­”æ¨¡å‹ã€‚\n"
            f"- è‹¥å†³å®šä¸å†è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œè¯·è°ƒç”¨ {self._exit_tool_name}(reason?) æ¥æ˜¾å¼é€€å‡ºå·¥å…·æ¨¡å¼ï¼Œç„¶ååœæ­¢ç»§ç»­è°ƒç”¨å…¶ä»–å·¥å…·ã€‚\n"
        )

    def _get_stream_system_prompt(self) -> str:
        """ç”¨äºæœ€ç»ˆå›ç­”é˜¶æ®µçš„ç³»ç»Ÿæç¤ºè¯ï¼šä¸“æ³¨äºé¢å‘ç”¨æˆ·çš„åˆ†æä¸ç”Ÿæˆï¼Œä¸è§¦å‘å·¥å…·è°ƒç”¨ã€‚"""
        now = datetime.now()
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
        return (
            f"ä»Šå¤©æ˜¯ {current_date}ï¼ˆ{current_weekday}ï¼‰ã€‚ä½ æ˜¯ä¸€ä¸ªå›ç­”åŠ©æ‰‹ã€‚\n"
            "- ä¸“æ³¨äºæ¸…æ™°ã€ç»“æ„åŒ–çš„ä¸­æ–‡å›ç­”ä¸åˆ†æã€‚\n"
            "- ä¸è¦è°ƒç”¨æˆ–æåŠä»»ä½•å·¥å…·æˆ–å‡½æ•°ã€‚\n"
            "- å¯ä»¥åˆ†æ¡è¯´æ˜ã€ç»™å‡ºç»“è®ºã€é£é™©ç‚¹ä¸åç»­å»ºè®®ã€‚\n"
        )

    def _sanitize_and_uniq_tool_name(self, name: str) -> str:
        """å°†å·¥å…·åè§„èŒƒä¸º ^[a-zA-Z0-9_-]+$ï¼Œå¹¶é¿å…é‡åå†²çªã€‚"""
        if not isinstance(name, str):
            name = str(name or "")
        # ä»…ä¿ç•™å­—æ¯æ•°å­—ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦ï¼Œå…¶ä½™æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        sanitized = re.sub(r"[^a-zA-Z0-9_-]", "_", name)
        if not sanitized:
            sanitized = "tool"
        base = sanitized
        # ç¡®ä¿å”¯ä¸€
        index = 1
        while sanitized in self._used_tool_names:
            index += 1
            sanitized = f"{base}_{index}"
        self._used_tool_names.add(sanitized)
        return sanitized

    async def chat_stream(self, user_input: str, history: List[Dict[str, Any]] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼æ¢æµ‹ + ç«‹å³ä¸­æ–­ï¼š
        - å…ˆç›´æ¥ astream å¼€æµï¼ŒçŸ­æš‚ç¼“å†²å¹¶æ£€æµ‹ function_call/tool_callï¼›
        - è‹¥æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼šç«‹å³ä¸­æ–­æœ¬æ¬¡æµå¼ï¼ˆä¸ä¸‹å‘ç¼“å†²ï¼‰ï¼Œæ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰ï¼Œå†™å› messages åè¿›å…¥ä¸‹ä¸€è½®ï¼›
        - è‹¥æœªæ£€æµ‹åˆ°å·¥å…·ï¼šå°†æœ¬æ¬¡æµä½œä¸ºæœ€ç»ˆå›ç­”ï¼Œå¼€å§‹æµå¼æ¨é€åˆ°ç»“æŸã€‚
        """
        try:
            print(f"ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
            yield {"type": "status", "content": "å¼€å§‹ç”Ÿæˆ..."}

            # 1) æ„å»ºå…±äº«æ¶ˆæ¯å†å²ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºï¼Œä¾¿äºä¸¤å¥—ç³»ç»Ÿæç¤ºåˆ†åˆ«æ³¨å…¥ï¼‰
            shared_history: List[Dict[str, Any]] = []
            if history:
                for record in history:
                    shared_history.append({"role": "user", "content": record['user_input']})
                    if record.get('ai_response'):
                        shared_history.append({"role": "assistant", "content": record['ai_response']})
            shared_history.append({"role": "user", "content": user_input})

            max_rounds = 5
            round_index = 0
            while round_index < max_rounds:
                round_index += 1
                print(f"ğŸ§  ç¬¬ {round_index} è½®æ¨ç† (åŒå®ä¾‹ï¼šåˆ¤å®šå·¥å…· + çº¯æµå¼å›ç­”)...")

                # 2) ä½¿ç”¨å¸¦å·¥å…·çš„å¸¸é©»å®ä¾‹åšåˆ¤å®šï¼ˆéæµå¼ï¼‰
                tools_messages = [{"role": "system", "content": self._get_tools_system_prompt()}] + shared_history
                try:
                    resp_check = await self.llm_tools.ainvoke(tools_messages)
                    tool_calls_check = getattr(resp_check, 'tool_calls', None)
                except Exception as e:
                    print(f"âš ï¸ å·¥å…·åˆ¤å®šå¤±è´¥ï¼Œé€€å›çº¯æµå¼ï¼š{e}")
                    tool_calls_check = None

                # è°ƒè¯•ï¼šæ‰“å°å¸¦å·¥å…·åˆ¤å®šé˜¶æ®µçš„LLMåŸå§‹è¾“å‡ºä¸å·¥å…·è°ƒç”¨å»ºè®®
                try:
                    content_preview = getattr(resp_check, 'content', None)
                    print("ğŸ“ å·¥å…·åˆ¤å®šé˜¶æ®µ LLM è¾“å‡º(content):")
                    print(content_preview if content_preview else "<empty>")

                    serialized_calls = []
                    if tool_calls_check:
                        for tc in tool_calls_check:
                            try:
                                if isinstance(tc, dict):
                                    fn = tc.get('function') or {}
                                    serialized_calls.append({
                                        "id": tc.get('id'),
                                        "name": fn.get('name') or tc.get('name'),
                                        "args_raw": fn.get('arguments') or tc.get('args'),
                                    })
                                else:
                                    # å…¼å®¹å¯¹è±¡å½¢å¼
                                    fn_obj = getattr(tc, 'function', None)
                                    args_raw = getattr(tc, 'args', None)
                                    if args_raw is None and fn_obj is not None:
                                        try:
                                            args_raw = getattr(fn_obj, 'arguments', None)
                                        except Exception:
                                            args_raw = None
                                    serialized_calls.append({
                                        "id": getattr(tc, 'id', None),
                                        "name": getattr(tc, 'name', ''),
                                        "args_raw": args_raw,
                                    })
                            except Exception as _e:
                                serialized_calls.append({"$raw": str(tc)})

                    print("ğŸ§© å·¥å…·åˆ¤å®šé˜¶æ®µ tool_calls (æ ‡å‡†åŒ–):")
                    try:
                        print(json.dumps(serialized_calls, ensure_ascii=False, indent=2))
                    except Exception:
                        print(str(serialized_calls))
                except Exception as log_e:
                    print(f"âš ï¸ æ‰“å°åˆ¤å®šé˜¶æ®µè¾“å‡ºå¤±è´¥: {log_e}")

                if tool_calls_check:
                    yield {"type": "tool_plan", "content": f"AIå†³å®šè°ƒç”¨ {len(tool_calls_check)} ä¸ªå·¥å…·", "tool_count": len(tool_calls_check)}
                    # å†™å›assistantå¸¦tool_calls
                    try:
                        shared_history.append({
                            "role": "assistant",
                            "content": getattr(resp_check, 'content', None) or "",
                            "tool_calls": tool_calls_check
                        })
                    except Exception:
                        shared_history.append({"role": "assistant", "content": getattr(resp_check, 'content', None) or ""})

                    # æ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰
                    exit_to_stream = False
                    for i, tool_call in enumerate(tool_calls_check, 1):
                        if isinstance(tool_call, dict):
                            tool_id = tool_call.get('id') or f"call_{i}"
                            fn = tool_call.get('function') or {}
                            tool_name = fn.get('name') or tool_call.get('name') or ''
                            tool_args_raw = fn.get('arguments') or tool_call.get('args') or {}
                        else:
                            tool_id = getattr(tool_call, 'id', None) or f"call_{i}"
                            tool_name = getattr(tool_call, 'name', '') or ''
                            tool_args_raw = getattr(tool_call, 'args', {}) or {}

                        # è§£æå‚æ•°
                        if isinstance(tool_args_raw, str):
                            try:
                                parsed_args = json.loads(tool_args_raw) if tool_args_raw else {}
                            except Exception:
                                parsed_args = {"$raw": tool_args_raw}
                        elif isinstance(tool_args_raw, dict):
                            parsed_args = tool_args_raw
                        else:
                            parsed_args = {"$raw": str(tool_args_raw)}

                        yield {"type": "tool_start", "tool_id": tool_id, "tool_name": tool_name, "tool_args": parsed_args, "progress": f"{i}/{len(tool_calls_check)}"}

                        try:
                            target_tool = None
                            for tool in self.tools:
                                if tool.name == tool_name:
                                    target_tool = tool
                                    break
                            if target_tool is None:
                                error_msg = f"å·¥å…· '{tool_name}' æœªæ‰¾åˆ°"
                                print(f"âŒ {error_msg}")
                                yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                                tool_result = f"é”™è¯¯: {error_msg}"
                            else:
                                tool_result = await target_tool.ainvoke(parsed_args)
                                yield {"type": "tool_end", "tool_id": tool_id, "tool_name": tool_name, "result": str(tool_result)}
                                # è‹¥ä¸ºæ˜¾å¼é€€å‡ºå·¥å…·æ¨¡å¼çš„å·¥å…·ï¼Œåˆ™æ ‡è®°å¹¶ä¸­æ–­åç»­å·¥å…·æ‰§è¡Œ
                                if tool_name == self._exit_tool_name:
                                    exit_to_stream = True
                                    # å°†ç®€çŸ­reasoné™„åŠ åˆ°æ—¥å¿—
                                    try:
                                        print(f"ğŸšª æ”¶åˆ°é€€å‡ºå·¥å…·æ¨¡å¼æŒ‡ä»¤: {parsed_args.get('reason', '')}")
                                    except Exception:
                                        pass
                        except Exception as e:
                            error_msg = f"å·¥å…·æ‰§è¡Œå‡ºé”™: {e}"
                            print(f"âŒ {error_msg}")
                            yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                            tool_result = f"é”™è¯¯: {error_msg}"

                        # å§‹ç»ˆè¿½åŠ  tool æ¶ˆæ¯ï¼Œæ»¡è¶³ OpenAI å‡½æ•°è°ƒç”¨åè®®è¦æ±‚
                        # å¯¹äºé€€å‡ºå·¥å…·æ¨¡å¼ï¼Œå†…å®¹ä¸ºç®€å•çŠ¶æ€ï¼Œä¸å½±å“åç»­å›ç­”è´¨é‡
                        shared_history.append({
                            "role": "tool",
                            "tool_call_id": tool_id,
                            "name": tool_name,
                            "content": str(tool_result)
                        })

                        if exit_to_stream:
                            break

                    if exit_to_stream:
                        # ç«‹å³è¿›å…¥æœ€ç»ˆå›ç­”çš„æµå¼è¾“å‡º
                        buffered_text = ""
                        response_started = False
                        final_text = ""

                        loop = asyncio.get_event_loop()
                        start_t = loop.time()
                        buffer_window_seconds = 0.5
                        min_flush_chars = 60

                        try:
                            stream_messages = [{"role": "system", "content": self._get_stream_system_prompt()}] + shared_history
                            async for event in self.llm_stream.astream_events(stream_messages, version="v1"):
                                ev = event.get("event")
                                if ev != "on_chat_model_stream":
                                    continue
                                data = event.get("data", {})
                                chunk = data.get("chunk")
                                if chunk is None:
                                    continue

                                try:
                                    content = getattr(chunk, 'content', None)
                                except Exception:
                                    content = None
                                if content:
                                    if not response_started:
                                        buffered_text += content
                                        time_elapsed = loop.time() - start_t
                                        if time_elapsed >= buffer_window_seconds or len(buffered_text) >= min_flush_chars:
                                            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                            yield {"type": "ai_response_chunk", "content": buffered_text}
                                            final_text += buffered_text
                                            buffered_text = ""
                                            response_started = True
                                    else:
                                        final_text += content
                                        yield {"type": "ai_response_chunk", "content": content}
                        except Exception as e:
                            print(f"âŒ å¤§æ¨¡å‹æµå¼ç”Ÿæˆå¤±è´¥: {e}")
                            yield {"type": "error", "content": f"å¤§æ¨¡å‹æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"}
                            return

                        if not response_started and buffered_text:
                            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                            yield {"type": "ai_response_chunk", "content": buffered_text}
                            final_text += buffered_text

                        yield {"type": "ai_response_end", "content": final_text}
                        return
                    else:
                        # å·¥å…·åç»§ç»­ä¸‹ä¸€è½®
                        continue

                # 3) æ— å·¥å…·ï¼šç”¨â€œæ— å·¥å…·å®ä¾‹â€åšçº¯æµå¼ï¼ˆä¸ä¼šäº§ç”Ÿ tool_calls å¢é‡ â†’ æ—  pydantic æŠ¥é”™ï¼‰
                #    åŒæ—¶ä¿ç•™çŸ­æš‚ç¼“å†²ï¼Œä¿è¯é¦–å±ç¨³å®š
                # 2) æµå¼æ¢æµ‹é˜¶æ®µï¼ˆçŸ­æš‚ç¼“å†²ï¼Œé¿å…å·¥å…·é˜¶æ®µæ–‡æœ¬ä¸‹å‘ï¼‰
                buffered_text = ""
                response_started = False
                final_text = ""

                # ä½¿ç”¨äº‹ä»¶å¾ªç¯æ—¶é—´æ¥åšçŸ­æš‚ç¼“å†²é˜ˆå€¼
                loop = asyncio.get_event_loop()
                start_t = loop.time()
                buffer_window_seconds = 0.5  # 500ms çª—å£
                min_flush_chars = 60         # æˆ–è€…æ–‡æœ¬è¾¾åˆ°ä¸€å®šé•¿åº¦å°±å¼€å§‹ä¸‹å‘

                try:
                    stream_messages = [{"role": "system", "content": self._get_stream_system_prompt()}] + shared_history
                    async for event in self.llm_stream.astream_events(stream_messages, version="v1"):
                        ev = event.get("event")
                        if ev != "on_chat_model_stream":
                            continue
                        data = event.get("data", {})
                        chunk = data.get("chunk")
                        if chunk is None:
                            continue

                        # æ–‡æœ¬å¤„ç†ï¼ˆç¼“å†² â†’ æ¡ä»¶åˆ·æ–° â†’ ç›´æ¥ä¸‹å‘ï¼‰
                        try:
                            content = getattr(chunk, 'content', None)
                        except Exception:
                            content = None
                        if content:
                            if not response_started:
                                buffered_text += content
                                time_elapsed = loop.time() - start_t
                                if time_elapsed >= buffer_window_seconds or len(buffered_text) >= min_flush_chars:
                                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                    yield {"type": "ai_response_chunk", "content": buffered_text}
                                    final_text += buffered_text
                                    buffered_text = ""
                                    response_started = True
                            else:
                                final_text += content
                                yield {"type": "ai_response_chunk", "content": content}
                except Exception as e:
                    print(f"âŒ å¤§æ¨¡å‹æµå¼ç”Ÿæˆå¤±è´¥: {e}")
                    yield {"type": "error", "content": f"å¤§æ¨¡å‹æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"}
                    return

                # æœªæ£€æµ‹åˆ°å·¥å…·ï¼šå¦‚æœè¿˜æ²¡å¼€å§‹ä¸‹å‘ï¼Œè¯´æ˜å…¨ç¨‹éƒ½åœ¨ç¼“å†²å†…ï¼Œç»Ÿä¸€ä½œä¸ºæœ€ç»ˆå›ç­”ä¸‹å‘
                if not response_started:
                    if buffered_text:
                        yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                        yield {"type": "ai_response_chunk", "content": buffered_text}
                        final_text += buffered_text

                yield {"type": "ai_response_end", "content": final_text}
                return

            # è½®æ¬¡è€—å°½ï¼šä¸å†æŠ¥é”™ï¼Œå›é€€åˆ°æœ€ç»ˆå›ç­”çš„æµå¼è¾“å‡º
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°({max_rounds})ï¼Œå›é€€ä¸ºç›´æ¥ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼ˆæ— å·¥å…·ï¼‰")
            try:
                buffered_text = ""
                response_started = False
                final_text = ""

                loop = asyncio.get_event_loop()
                start_t = loop.time()
                buffer_window_seconds = 0.5
                min_flush_chars = 60

                stream_messages = [{"role": "system", "content": self._get_stream_system_prompt()}] + shared_history
                async for event in self.llm_stream.astream_events(stream_messages, version="v1"):
                    ev = event.get("event")
                    if ev != "on_chat_model_stream":
                        continue
                    data = event.get("data", {})
                    chunk = data.get("chunk")
                    if chunk is None:
                        continue

                    try:
                        content = getattr(chunk, 'content', None)
                    except Exception:
                        content = None
                    if content:
                        if not response_started:
                            buffered_text += content
                            time_elapsed = loop.time() - start_t
                            if time_elapsed >= buffer_window_seconds or len(buffered_text) >= min_flush_chars:
                                yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                yield {"type": "ai_response_chunk", "content": buffered_text}
                                final_text += buffered_text
                                buffered_text = ""
                                response_started = True
                        else:
                            final_text += content
                            yield {"type": "ai_response_chunk", "content": content}

                if not response_started and buffered_text:
                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                    yield {"type": "ai_response_chunk", "content": buffered_text}
                    final_text += buffered_text

                yield {"type": "ai_response_end", "content": final_text}
                return
            except Exception as e:
                print(f"âŒ å›é€€æµå¼è¾“å‡ºå¤±è´¥: {e}")
                yield {"type": "error", "content": f"è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°ï¼Œä¸”å›é€€ç”Ÿæˆå¤±è´¥: {str(e)}"}
        except Exception as e:
            import traceback
            print(f"âŒ chat_stream å¼‚å¸¸: {e}")
            print("ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            yield {"type": "error", "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"}

    def get_tools_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰MCPæœåŠ¡å™¨åˆ†ç»„"""
        if not self.tools_by_server:
            return {"servers": {}, "total_tools": 0, "server_count": 0}
        
        servers_info = {}
        total_tools = 0
        
        # æŒ‰æœåŠ¡å™¨åˆ†ç»„æ„å»ºå·¥å…·ä¿¡æ¯
        for server_name, server_tools in self.tools_by_server.items():
            tools_info = []
            
            for tool in server_tools:
                tool_info = {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": {},
                    "required": []
                }
                
                # è·å–å‚æ•°ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬
                try:
                    schema = None
                    
                    # æ–¹æ³•1: å°è¯•ä½¿ç”¨args_schema (LangChainå·¥å…·å¸¸ç”¨)
                    if hasattr(tool, 'args_schema') and tool.args_schema:
                        if isinstance(tool.args_schema, dict):
                            schema = tool.args_schema
                        elif hasattr(tool.args_schema, 'model_json_schema'):
                            schema = tool.args_schema.model_json_schema()
                    
                    # æ–¹æ³•2: å¦‚æœæ²¡æœ‰args_schemaï¼Œå°è¯•tool_call_schema
                    if not schema and hasattr(tool, 'tool_call_schema') and tool.tool_call_schema:
                        schema = tool.tool_call_schema
                    
                    # æ–¹æ³•3: æœ€åå°è¯•input_schema
                    if not schema and hasattr(tool, 'input_schema') and tool.input_schema:
                        if isinstance(tool.input_schema, dict):
                            schema = tool.input_schema
                        elif hasattr(tool.input_schema, 'model_json_schema'):
                            try:
                                schema = tool.input_schema.model_json_schema()
                            except:
                                pass
                    
                    # è§£æschema
                    if schema and isinstance(schema, dict):
                        if 'properties' in schema:
                            tool_info["parameters"] = schema['properties']
                            tool_info["required"] = schema.get('required', [])
                        elif 'type' in schema and schema.get('type') == 'object' and 'properties' in schema:
                            tool_info["parameters"] = schema['properties']
                            tool_info["required"] = schema.get('required', [])
                
                except Exception as e:
                    # å¦‚æœå‡ºé”™ï¼Œè‡³å°‘ä¿ç•™å·¥å…·çš„åŸºæœ¬ä¿¡æ¯
                    print(f"âš ï¸ è·å–å·¥å…· '{tool.name}' å‚æ•°ä¿¡æ¯å¤±è´¥: {e}")
                
                tools_info.append(tool_info)
            
            # æ·»åŠ æœåŠ¡å™¨ä¿¡æ¯
            servers_info[server_name] = {
                "name": server_name,
                "tools": tools_info,
                "tool_count": len(tools_info)
            }
            
            total_tools += len(tools_info)
        
        return {
            "servers": servers_info,
            "total_tools": total_tools,
            "server_count": len(servers_info)
        }

    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self.mcp_client and hasattr(self.mcp_client, 'close'):
                await self.mcp_client.close()
        except:
            pass
